{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d76c2b",
   "metadata": {},
   "source": [
    "# Course Project — UCI Credit Default (Simple)\n",
    "\n",
    "This notebook follows the **course project guidelines** (Parts A–E) using the dataset `UCI_Credit_Card.csv`. It includes:\n",
    "- Data understanding + basic stats + plots (Part A)\n",
    "- PCA from scratch + sklearn comparison (Part B)\n",
    "- Linear Regression with Gradient Descent from scratch (Part C)\n",
    "- Logistic Regression from scratch (extra, helps for classification)\n",
    "- Statistical evaluation: metrics, t-test vs baseline, bootstrap CI (Part E)\n",
    "\n",
    "> Note: The original guideline also asks for **Perceptron + MLP**. If you want, I can add them in the same style later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503b807",
   "metadata": {},
   "source": [
    "## 0) Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA as SKPCA\n",
    "from sklearn.linear_model import LinearRegression as SKLinearRegression\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"UCI_Credit_Card.csv\")\n",
    "df.shape, df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24da9f",
   "metadata": {},
   "source": [
    "## A) Data Understanding & Preprocessing (15%)\n",
    "- Describe dataset\n",
    "- Handle missing/outliers\n",
    "- Normalize features\n",
    "- Compute mean/variance/covariance/correlation\n",
    "- 2–3 plots + short notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look\n",
    "display(df.head())\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df[\"default.payment.next.month\"].value_counts(normalize=True).rename(\"class_ratio\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc536ce2",
   "metadata": {},
   "source": [
    "### A1) Clean obvious category issues (simple)\n",
    "In this dataset, some category codes are unusual (common in UCI version):\n",
    "- EDUCATION: values like 0, 5, 6 can be grouped into 'other'\n",
    "- MARRIAGE: value 0 can be grouped into 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6161aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "# EDUCATION: {0,5,6} -> 4 (other). Keep 1..4\n",
    "df_clean[\"EDUCATION\"] = df_clean[\"EDUCATION\"].replace({0:4, 5:4, 6:4})\n",
    "\n",
    "# MARRIAGE: {0} -> 3 (other). Keep 1..3\n",
    "df_clean[\"MARRIAGE\"] = df_clean[\"MARRIAGE\"].replace({0:3})\n",
    "\n",
    "# Missing values?\n",
    "print(\"Missing values per column (top 10):\")\n",
    "print(df_clean.isna().sum().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429285c1",
   "metadata": {},
   "source": [
    "### A2) Handle outliers (winsorization)\n",
    "Simple method: clip each numeric column to the 1st and 99th percentiles.\n",
    "This reduces extreme values but keeps all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37da9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_df(dataframe, cols, lower_q=0.01, upper_q=0.99):\n",
    "    dfw = dataframe.copy()\n",
    "    for c in cols:\n",
    "        lo, hi = dfw[c].quantile(lower_q), dfw[c].quantile(upper_q)\n",
    "        dfw[c] = dfw[c].clip(lo, hi)\n",
    "    return dfw\n",
    "\n",
    "numeric_cols = [c for c in df_clean.columns if c not in [\"ID\"]]\n",
    "df_w = winsorize_df(df_clean, numeric_cols, 0.01, 0.99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063a609",
   "metadata": {},
   "source": [
    "### A3) Basic statistics: mean, variance, covariance, correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll compute stats on the features only (exclude ID and target)\n",
    "target_col = \"default.payment.next.month\"\n",
    "feature_cols = [c for c in df_w.columns if c not in [\"ID\", target_col]]\n",
    "\n",
    "X_all = df_w[feature_cols].values.astype(float)\n",
    "\n",
    "mean_vec = X_all.mean(axis=0)\n",
    "var_vec  = X_all.var(axis=0, ddof=1)  # sample variance\n",
    "\n",
    "cov_mat = np.cov(X_all, rowvar=False)  # covariance matrix\n",
    "corr_mat = np.corrcoef(X_all, rowvar=False)  # correlation matrix\n",
    "\n",
    "print(\"Mean vector shape:\", mean_vec.shape)\n",
    "print(\"Cov matrix shape:\", cov_mat.shape)\n",
    "print(\"Corr matrix shape:\", corr_mat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757a47b",
   "metadata": {},
   "source": [
    "### A4) Visualizations (2–3 plots)\n",
    "- Histogram of target (class balance)\n",
    "- Histogram of LIMIT_BAL\n",
    "- Scatter: AGE vs LIMIT_BAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Target histogram (bar)\n",
    "counts = df_w[target_col].value_counts().sort_index()\n",
    "plt.figure()\n",
    "plt.bar(counts.index.astype(str), counts.values)\n",
    "plt.title(\"Target distribution (0=No default, 1=Default)\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 2) Histogram of LIMIT_BAL\n",
    "plt.figure()\n",
    "plt.hist(df_w[\"LIMIT_BAL\"].values, bins=50)\n",
    "plt.title(\"Histogram: LIMIT_BAL (Credit Limit)\")\n",
    "plt.xlabel(\"LIMIT_BAL\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# 3) Scatter AGE vs LIMIT_BAL\n",
    "plt.figure()\n",
    "plt.scatter(df_w[\"AGE\"].values, df_w[\"LIMIT_BAL\"].values, s=5, alpha=0.3)\n",
    "plt.title(\"Scatter: AGE vs LIMIT_BAL\")\n",
    "plt.xlabel(\"AGE\")\n",
    "plt.ylabel(\"LIMIT_BAL\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6a88bf",
   "metadata": {},
   "source": [
    "## B) PCA from Scratch (15%)\n",
    "- Implement PCA using eigen decomposition\n",
    "- Scree plot\n",
    "- Project to 2D and scatter plot\n",
    "- Compare with sklearn PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8534602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_from_scratch(X, k=None):\n",
    "    \"\"\"Return (components, explained_variance_ratio, X_projected)\"\"\"\n",
    "    # Center\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    # Covariance\n",
    "    C = (Xc.T @ Xc) / (Xc.shape[0] - 1)\n",
    "    # Eigen decomposition (C is symmetric)\n",
    "    eigvals, eigvecs = np.linalg.eigh(C)   # ascending\n",
    "    order = np.argsort(eigvals)[::-1]      # descending\n",
    "    eigvals = eigvals[order]\n",
    "    eigvecs = eigvecs[:, order]\n",
    "    # Explained variance ratio\n",
    "    evr = eigvals / eigvals.sum()\n",
    "    if k is None:\n",
    "        k = X.shape[1]\n",
    "    W = eigvecs[:, :k]         # top-k eigenvectors\n",
    "    X_proj = Xc @ W\n",
    "    return W, evr, X_proj\n",
    "\n",
    "# Standardize features before PCA\n",
    "scaler_pca = StandardScaler()\n",
    "X_scaled = scaler_pca.fit_transform(df_w[feature_cols].values.astype(float))\n",
    "\n",
    "W, evr, X2 = pca_from_scratch(X_scaled, k=2)\n",
    "\n",
    "# Scree plot (first 15 components)\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, min(16, len(evr))+1), evr[:15], marker=\"o\")\n",
    "plt.title(\"Scree Plot (Explained Variance Ratio) — Scratch PCA\")\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Explained variance ratio\")\n",
    "plt.show()\n",
    "\n",
    "# 2D projection scatter\n",
    "plt.figure()\n",
    "plt.scatter(X2[:,0], X2[:,1], s=5, alpha=0.3)\n",
    "plt.title(\"2D Projection — Scratch PCA (k=2)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "# Compare with sklearn PCA (k=2)\n",
    "skpca = SKPCA(n_components=2, random_state=42)\n",
    "X2_sk = skpca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Scratch EVR (first 2):\", evr[:2])\n",
    "print(\"sklearn EVR (first 2):\", skpca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee949dec",
   "metadata": {},
   "source": [
    "## C) Linear Regression with Gradient Descent (20%)\n",
    "We need a **regression target** (continuous). The dataset’s main target is classification (default).\n",
    "So for Part C, we’ll predict a continuous column:\n",
    "- **y_reg = BILL_AMT1** (first bill amount)\n",
    "This lets us compute MSE/MAE/R² and compare with sklearn LinearRegression.\n",
    "\n",
    "Deliverables:\n",
    "- Loss vs epochs (for different learning rates)\n",
    "- Coefficient comparison vs sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7077710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare regression data\n",
    "y_reg = df_w[\"BILL_AMT1\"].values.astype(float)\n",
    "\n",
    "X_reg = df_w[feature_cols].values.astype(float)  # use same features (excluding ID/target)\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features for GD stability\n",
    "scaler_reg = StandardScaler()\n",
    "Xr_train_s = scaler_reg.fit_transform(Xr_train)\n",
    "Xr_test_s  = scaler_reg.transform(Xr_test)\n",
    "\n",
    "def add_bias(X):\n",
    "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "Xr_train_b = add_bias(Xr_train_s)\n",
    "Xr_test_b  = add_bias(Xr_test_s)\n",
    "\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, lr=0.01, epochs=500):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.theta = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, Xb, y):\n",
    "        n, d = Xb.shape\n",
    "        self.theta = np.zeros(d)\n",
    "        for _ in range(self.epochs):\n",
    "            yhat = Xb @ self.theta\n",
    "            error = yhat - y\n",
    "            # MSE loss\n",
    "            loss = (error**2).mean()\n",
    "            self.loss_history.append(loss)\n",
    "            # Gradient\n",
    "            grad = (2/n) * (Xb.T @ error)\n",
    "            self.theta -= self.lr * grad\n",
    "        return self\n",
    "\n",
    "    def predict(self, Xb):\n",
    "        return Xb @ self.theta\n",
    "\n",
    "# Train with different learning rates\n",
    "lrs = [1e-4, 5e-4, 1e-3]\n",
    "models = []\n",
    "for lr in lrs:\n",
    "    m = LinearRegressionGD(lr=lr, epochs=400).fit(Xr_train_b, yr_train)\n",
    "    models.append(m)\n",
    "\n",
    "plt.figure()\n",
    "for lr, m in zip(lrs, models):\n",
    "    plt.plot(m.loss_history, label=f\"lr={lr}\")\n",
    "plt.title(\"Linear Regression (GD) — Loss vs Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Pick best (lowest final loss)\n",
    "best_idx = int(np.argmin([m.loss_history[-1] for m in models]))\n",
    "best_lr = lrs[best_idx]\n",
    "lin_gd = models[best_idx]\n",
    "print(\"Best learning rate:\", best_lr)\n",
    "\n",
    "# Evaluate\n",
    "yr_pred = lin_gd.predict(Xr_test_b)\n",
    "print(\"Scratch Linear Regression:\")\n",
    "print(\"MSE:\", mean_squared_error(yr_test, yr_pred))\n",
    "print(\"MAE:\", mean_absolute_error(yr_test, yr_pred))\n",
    "print(\"R2 :\", r2_score(yr_test, yr_pred))\n",
    "\n",
    "# Compare with sklearn LinearRegression\n",
    "sk_lin = SKLinearRegression()\n",
    "sk_lin.fit(Xr_train_s, yr_train)\n",
    "yr_pred_sk = sk_lin.predict(Xr_test_s)\n",
    "\n",
    "print(\"\\nSklearn LinearRegression:\")\n",
    "print(\"MSE:\", mean_squared_error(yr_test, yr_pred_sk))\n",
    "print(\"MAE:\", mean_absolute_error(yr_test, yr_pred_sk))\n",
    "print(\"R2 :\", r2_score(yr_test, yr_pred_sk))\n",
    "\n",
    "# Coefficient comparison (first 10)\n",
    "print(\"\\nCoefficient comparison (first 10 features):\")\n",
    "print(\"Scratch theta (bias + first 9):\", lin_gd.theta[:10])\n",
    "print(\"Sklearn coef  (first 10):      \", np.r_[sk_lin.intercept_, sk_lin.coef_[:9]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c0bdd",
   "metadata": {},
   "source": [
    "## Logistic Regression from Scratch (Simple)\n",
    "This targets the real label: `default.payment.next.month` (binary classification).\n",
    "We implement logistic regression with gradient descent.\n",
    "\n",
    "Deliverables:\n",
    "- Confusion matrix\n",
    "- Accuracy / Precision / Recall\n",
    "- Compare to sklearn LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc9dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare classification data\n",
    "y_cls = df_w[target_col].values.astype(int)\n",
    "X_cls = df_w[feature_cols].values.astype(float)\n",
    "\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42, stratify=y_cls)\n",
    "\n",
    "# Scale features\n",
    "scaler_cls = StandardScaler()\n",
    "Xc_train_s = scaler_cls.fit_transform(Xc_train)\n",
    "Xc_test_s  = scaler_cls.transform(Xc_test)\n",
    "\n",
    "Xc_train_b = add_bias(Xc_train_s)\n",
    "Xc_test_b  = add_bias(Xc_test_s)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    def __init__(self, lr=0.05, epochs=800, reg_lambda=0.0):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.theta = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, Xb, y):\n",
    "        n, d = Xb.shape\n",
    "        self.theta = np.zeros(d)\n",
    "        for _ in range(self.epochs):\n",
    "            z = Xb @ self.theta\n",
    "            p = sigmoid(z)\n",
    "            # Logistic loss (with small epsilon for stability)\n",
    "            eps = 1e-12\n",
    "            loss = -(y*np.log(p+eps) + (1-y)*np.log(1-p+eps)).mean()\n",
    "            # L2 regularization (exclude bias)\n",
    "            if self.reg_lambda > 0:\n",
    "                loss += (self.reg_lambda/(2*n)) * np.sum(self.theta[1:]**2)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            # Gradient\n",
    "            grad = (1/n) * (Xb.T @ (p - y))\n",
    "            if self.reg_lambda > 0:\n",
    "                reg = (self.reg_lambda/n) * self.theta\n",
    "                reg[0] = 0.0\n",
    "                grad += reg\n",
    "            self.theta -= self.lr * grad\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, Xb):\n",
    "        return sigmoid(Xb @ self.theta)\n",
    "\n",
    "    def predict(self, Xb, threshold=0.5):\n",
    "        return (self.predict_proba(Xb) >= threshold).astype(int)\n",
    "\n",
    "log_gd = LogisticRegressionGD(lr=0.05, epochs=800, reg_lambda=0.1).fit(Xc_train_b, yc_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(log_gd.loss_history)\n",
    "plt.title(\"Logistic Regression (GD) — Loss vs Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.show()\n",
    "\n",
    "yc_pred = log_gd.predict(Xc_test_b, threshold=0.5)\n",
    "\n",
    "print(\"Scratch Logistic Regression:\")\n",
    "print(\"Accuracy :\", accuracy_score(yc_test, yc_pred))\n",
    "print(\"Precision:\", precision_score(yc_test, yc_pred, zero_division=0))\n",
    "print(\"Recall   :\", recall_score(yc_test, yc_pred, zero_division=0))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(yc_test, yc_pred))\n",
    "\n",
    "# Compare with sklearn LogisticRegression\n",
    "sk_log = SKLogisticRegression(max_iter=2000)\n",
    "sk_log.fit(Xc_train_s, yc_train)\n",
    "yc_pred_sk = sk_log.predict(Xc_test_s)\n",
    "\n",
    "print(\"\\nSklearn LogisticRegression:\")\n",
    "print(\"Accuracy :\", accuracy_score(yc_test, yc_pred_sk))\n",
    "print(\"Precision:\", precision_score(yc_test, yc_pred_sk, zero_division=0))\n",
    "print(\"Recall   :\", recall_score(yc_test, yc_pred_sk, zero_division=0))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(yc_test, yc_pred_sk))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128840ae",
   "metadata": {},
   "source": [
    "## E) Statistical Evaluation (15%)\n",
    "Guidelines ask for:\n",
    "- Regression: MSE, MAE, R² ✅ (done above)\n",
    "- Classification: Accuracy, Precision, Recall ✅ (done above)\n",
    "- **t-test comparing model to baseline**\n",
    "- **Bootstrap confidence interval for accuracy**\n",
    "\n",
    "We’ll do:\n",
    "1) Baseline classifier: predict the majority class from training.\n",
    "2) 5-fold CV on a **subset** for speed.\n",
    "3) Paired t-test on fold accuracies.\n",
    "4) Bootstrap 95% CI on test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ed816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Baseline (majority class) on the test set ---\n",
    "majority_class = int(pd.Series(yc_train).mode()[0])\n",
    "baseline_pred = np.full_like(yc_test, majority_class)\n",
    "acc_baseline_test = accuracy_score(yc_test, baseline_pred)\n",
    "acc_model_test = accuracy_score(yc_test, yc_pred)\n",
    "\n",
    "print(\"Test Accuracy — Baseline:\", acc_baseline_test)\n",
    "print(\"Test Accuracy — Scratch Logistic:\", acc_model_test)\n",
    "\n",
    "# --- 2) 5-fold CV (subset for speed) ---\n",
    "def kfold_indices(n, k=5, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n)\n",
    "    rng.shuffle(idx)\n",
    "    folds = np.array_split(idx, k)\n",
    "    return folds\n",
    "\n",
    "# Use a smaller subset for quick CV\n",
    "n_sub = 6000\n",
    "sub_idx = np.random.default_rng(42).choice(len(X_cls), size=n_sub, replace=False)\n",
    "X_sub = X_cls[sub_idx]\n",
    "y_sub = y_cls[sub_idx]\n",
    "\n",
    "# scale inside each fold (important)\n",
    "folds = kfold_indices(n_sub, k=5, seed=42)\n",
    "\n",
    "acc_model_folds = []\n",
    "acc_base_folds = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_idx = folds[i]\n",
    "    train_idx = np.hstack([folds[j] for j in range(5) if j != i])\n",
    "\n",
    "    Xtr, ytr = X_sub[train_idx], y_sub[train_idx]\n",
    "    Xte, yte = X_sub[test_idx], y_sub[test_idx]\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    Xtr_s = sc.fit_transform(Xtr)\n",
    "    Xte_s = sc.transform(Xte)\n",
    "\n",
    "    Xtr_b = add_bias(Xtr_s)\n",
    "    Xte_b = add_bias(Xte_s)\n",
    "\n",
    "    # Baseline majority on this fold\n",
    "    maj = int(pd.Series(ytr).mode()[0])\n",
    "    y_base = np.full_like(yte, maj)\n",
    "    acc_base_folds.append(accuracy_score(yte, y_base))\n",
    "\n",
    "    # Train scratch logistic quickly (fewer epochs for CV)\n",
    "    m = LogisticRegressionGD(lr=0.05, epochs=400, reg_lambda=0.1).fit(Xtr_b, ytr)\n",
    "    yhat = m.predict(Xte_b)\n",
    "    acc_model_folds.append(accuracy_score(yte, yhat))\n",
    "\n",
    "acc_model_folds = np.array(acc_model_folds)\n",
    "acc_base_folds = np.array(acc_base_folds)\n",
    "\n",
    "print(\"Fold Accuracies (model):   \", acc_model_folds)\n",
    "print(\"Fold Accuracies (baseline):\", acc_base_folds)\n",
    "print(\"Mean model acc:\", acc_model_folds.mean(), \" | Mean baseline acc:\", acc_base_folds.mean())\n",
    "\n",
    "# --- 3) Paired t-test (if SciPy exists) ---\n",
    "try:\n",
    "    from scipy import stats\n",
    "    t_stat, p_val = stats.ttest_rel(acc_model_folds, acc_base_folds)\n",
    "    print(\"Paired t-test: t =\", t_stat, \", p =\", p_val)\n",
    "except Exception as e:\n",
    "    print(\"SciPy not available for t-test. Error:\", e)\n",
    "    # simple fallback: just print the differences\n",
    "    diffs = acc_model_folds - acc_base_folds\n",
    "    print(\"Differences per fold:\", diffs, \" | mean diff:\", diffs.mean())\n",
    "\n",
    "# --- 4) Bootstrap 95% CI for test accuracy ---\n",
    "rng = np.random.default_rng(42)\n",
    "B = 2000\n",
    "acc_samples = []\n",
    "n_test = len(yc_test)\n",
    "for _ in range(B):\n",
    "    bs_idx = rng.integers(0, n_test, size=n_test)\n",
    "    acc_samples.append(accuracy_score(yc_test[bs_idx], yc_pred[bs_idx]))\n",
    "acc_samples = np.array(acc_samples)\n",
    "ci_low, ci_high = np.percentile(acc_samples, [2.5, 97.5])\n",
    "print(f\"Bootstrap 95% CI for Scratch Logistic test accuracy: [{ci_low:.4f}, {ci_high:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f670f8",
   "metadata": {},
   "source": [
    "## Summary Table (Quick)\n",
    "This is a small summary you can copy to your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({\n",
    "    \"Task\": [\"Regression (BILL_AMT1)\", \"Classification (Default)\"],\n",
    "    \"Model\": [\"Linear Regression (GD scratch)\", \"Logistic Regression (GD scratch)\"],\n",
    "    \"Main Metrics\": [\n",
    "        f\"MSE={mean_squared_error(yr_test, yr_pred):.2f}, MAE={mean_absolute_error(yr_test, yr_pred):.2f}, R2={r2_score(yr_test, yr_pred):.4f}\",\n",
    "        f\"Acc={accuracy_score(yc_test, yc_pred):.4f}, Prec={precision_score(yc_test, yc_pred, zero_division=0):.4f}, Rec={recall_score(yc_test, yc_pred, zero_division=0):.4f}\"\n",
    "    ]\n",
    "})\n",
    "display(summary)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
